{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c544cce5",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding (BPE)最初是一种文本压缩算法在15年被引入到NLP用于分词[3]，在训练 GPT 时被 OpenAI 用于tokenization，后续好多模型GPT，RoBERTa等都采用了这种分词方法。Byte-level BPE(BBPE)是于19年在BPE的基础上提出以Byte-level(字节)为粒度的分词方法[4]，目前GPT2，BLOOM，Llama，Falcon等采用的是该分词方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de15a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"我\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果\",\n",
    "    \"他\",\n",
    "    \"不\",\n",
    "    \"喜欢\",\n",
    "    \"吃\",\n",
    "    \"苹果派\",\n",
    "    \"I like to eat apples\",\n",
    "    \"She has a cute cat\",\n",
    "    \"you are very cute\",\n",
    "    \"give you a hug\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8668f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: defaultdict(<class 'int'>, {'我': 1, '喜欢': 2, '吃': 2, '苹果': 1, '他': 1, '不': 1, '苹果派': 1, 'I': 1, 'like': 1, 'to': 1, 'eat': 1, 'apples': 1, 'She': 1, 'has': 1, 'a': 2, 'cute': 2, 'cat': 1, 'you': 2, 'are': 1, 'very': 1, 'give': 1, 'hug': 1})\n",
      "alphabet: ['I', 'S', 'a', 'c', 'e', 'g', 'h', 'i', 'k', 'l', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', '不', '他', '吃', '喜', '我', '果', '欢', '派', '苹']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 构建频率统计\n",
    "def build_stats(sentences):\n",
    "    stats = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        symbols = sentence.split()\n",
    "        for symbol in symbols:\n",
    "            stats[symbol] += 1\n",
    "    return stats\n",
    "\n",
    "stats = build_stats(sentences)\n",
    "print(\"stats:\", stats)\n",
    "\n",
    "alphabet = []\n",
    "for word in stats.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "# 初始词表\n",
    "vocab = alphabet.copy()\n",
    "print(\"alphabet:\", alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "746a02c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits: {'我': ['我'], '喜欢': ['喜', '欢'], '吃': ['吃'], '苹果': ['苹', '果'], '他': ['他'], '不': ['不'], '苹果派': ['苹', '果', '派'], 'I': ['I'], 'like': ['l', 'i', 'k', 'e'], 'to': ['t', 'o'], 'eat': ['e', 'a', 't'], 'apples': ['a', 'p', 'p', 'l', 'e', 's'], 'She': ['S', 'h', 'e'], 'has': ['h', 'a', 's'], 'a': ['a'], 'cute': ['c', 'u', 't', 'e'], 'cat': ['c', 'a', 't'], 'you': ['y', 'o', 'u'], 'are': ['a', 'r', 'e'], 'very': ['v', 'e', 'r', 'y'], 'give': ['g', 'i', 'v', 'e'], 'hug': ['h', 'u', 'g']}\n",
      "('喜', '欢'): 2\n",
      "('苹', '果'): 2\n",
      "('果', '派'): 1\n",
      "('l', 'i'): 1\n",
      "('i', 'k'): 1\n",
      "('k', 'e'): 1\n",
      "('t', 'o'): 1\n",
      "('e', 'a'): 1\n",
      "('a', 't'): 2\n",
      "('a', 'p'): 1\n",
      "('p', 'p'): 1\n",
      "('p', 'l'): 1\n",
      "('l', 'e'): 1\n",
      "('e', 's'): 1\n",
      "('S', 'h'): 1\n",
      "('h', 'e'): 1\n",
      "('h', 'a'): 1\n",
      "('a', 's'): 1\n",
      "('c', 'u'): 2\n",
      "('u', 't'): 2\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in stats.keys()}\n",
    "print(\"splits:\", splits)\n",
    "\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in stats.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 19:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba68d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('喜', '欢') 2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0acce14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in stats:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f0200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges: {('喜', '欢'): '喜欢', ('苹', '果'): '苹果', ('a', 't'): 'at', ('c', 'u'): 'cu', ('cu', 't'): 'cut', ('cut', 'e'): 'cute', ('y', 'o'): 'yo', ('yo', 'u'): 'you', ('v', 'e'): 've', ('苹果', '派'): '苹果派', ('l', 'i'): 'li', ('li', 'k'): 'lik', ('lik', 'e'): 'like', ('t', 'o'): 'to', ('e', 'at'): 'eat', ('a', 'p'): 'ap', ('ap', 'p'): 'app', ('app', 'l'): 'appl', ('appl', 'e'): 'apple', ('apple', 's'): 'apples', ('S', 'h'): 'Sh', ('Sh', 'e'): 'She', ('h', 'a'): 'ha'}\n",
      "vocab: ['I', 'S', 'a', 'c', 'e', 'g', 'h', 'i', 'k', 'l', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', '不', '他', '吃', '喜', '我', '果', '欢', '派', '苹', '喜欢', '苹果', 'at', 'cu', 'cut', 'cute', 'yo', 'you', 've', '苹果派', 'li', 'lik', 'like', 'to', 'eat', 'ap', 'app', 'appl', 'apple', 'apples', 'Sh', 'She', 'ha']\n"
     ]
    }
   ],
   "source": [
    "# 假设我们想要的词典为50\n",
    "merges = {}\n",
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "print(\"merges:\", merges)\n",
    "print(\"vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333cf06",
   "metadata": {},
   "source": [
    "BPE理论上还是会出现OOV的，当词汇表的大小受限时，一些较少频繁出现的子词和没有在训练过程中见过的子词，就会无法进入词汇表出现OOV，而Byte-level BPE(BBPE)理论上是不会出现这个情况的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
