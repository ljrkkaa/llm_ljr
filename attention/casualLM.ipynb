{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8986e1ef",
   "metadata": {},
   "source": [
    "transformers decoder 的流程是：input -> self-attention -> cross-attention -> FFN\n",
    "\n",
    "causalLM decoder 的流程是 input -> self-attention -> FFN\n",
    "\n",
    "其他 [self-attention, FFN] 是一个 block，一般会有很多的 block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9bbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47621cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4892, 0.5108, 0.0000, 0.0000],\n",
      "          [0.3325, 0.3386, 0.3289, 0.0000],\n",
      "          [0.2376, 0.2553, 0.2500, 0.2571]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4835, 0.5165, 0.0000, 0.0000],\n",
      "          [0.3339, 0.3202, 0.3459, 0.0000],\n",
      "          [0.2486, 0.2504, 0.2521, 0.2489]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5001, 0.4999, 0.0000, 0.0000],\n",
      "          [0.3510, 0.3270, 0.3220, 0.0000],\n",
      "          [0.2455, 0.2532, 0.2394, 0.2619]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4984, 0.5016, 0.0000, 0.0000],\n",
      "          [0.3405, 0.3416, 0.3179, 0.0000],\n",
      "          [0.2666, 0.2516, 0.2394, 0.2424]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4431, 0.5569, 0.0000, 0.0000],\n",
      "          [0.2987, 0.3589, 0.3424, 0.0000],\n",
      "          [0.2236, 0.2610, 0.2540, 0.2613]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5015, 0.4985, 0.0000, 0.0000],\n",
      "          [0.3243, 0.3293, 0.3464, 0.0000],\n",
      "          [0.2508, 0.2478, 0.2526, 0.2488]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5052, 0.4948, 0.0000, 0.0000],\n",
      "          [0.3280, 0.3257, 0.3463, 0.0000],\n",
      "          [0.2504, 0.2370, 0.2562, 0.2564]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4700, 0.5300, 0.0000, 0.0000],\n",
      "          [0.3114, 0.3545, 0.3341, 0.0000],\n",
      "          [0.2304, 0.2800, 0.2494, 0.2402]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5121, 0.4879, 0.0000, 0.0000],\n",
      "          [0.5141, 0.4859, 0.0000, 0.0000],\n",
      "          [0.5198, 0.4802, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5068, 0.4932, 0.0000, 0.0000],\n",
      "          [0.5013, 0.4987, 0.0000, 0.0000],\n",
      "          [0.4984, 0.5016, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5105, 0.4895, 0.0000, 0.0000],\n",
      "          [0.4991, 0.5009, 0.0000, 0.0000],\n",
      "          [0.5084, 0.4916, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4740, 0.5260, 0.0000, 0.0000],\n",
      "          [0.4992, 0.5008, 0.0000, 0.0000],\n",
      "          [0.4977, 0.5023, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4736, 0.5264, 0.0000, 0.0000],\n",
      "          [0.4762, 0.5238, 0.0000, 0.0000],\n",
      "          [0.4926, 0.5074, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5142, 0.4858, 0.0000, 0.0000],\n",
      "          [0.5138, 0.4862, 0.0000, 0.0000],\n",
      "          [0.5065, 0.4935, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4899, 0.5101, 0.0000, 0.0000],\n",
      "          [0.4816, 0.5184, 0.0000, 0.0000],\n",
      "          [0.4899, 0.5101, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5329, 0.4671, 0.0000, 0.0000],\n",
      "          [0.4976, 0.5024, 0.0000, 0.0000],\n",
      "          [0.5039, 0.4961, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5060, 0.4940, 0.0000, 0.0000],\n",
      "          [0.3237, 0.3331, 0.3432, 0.0000],\n",
      "          [0.3361, 0.3273, 0.3366, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5076, 0.4924, 0.0000, 0.0000],\n",
      "          [0.3437, 0.3370, 0.3193, 0.0000],\n",
      "          [0.3493, 0.3266, 0.3240, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5104, 0.4896, 0.0000, 0.0000],\n",
      "          [0.3367, 0.3298, 0.3335, 0.0000],\n",
      "          [0.3269, 0.3368, 0.3363, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5110, 0.4890, 0.0000, 0.0000],\n",
      "          [0.3406, 0.3263, 0.3331, 0.0000],\n",
      "          [0.3357, 0.3188, 0.3455, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5061, 0.4939, 0.0000, 0.0000],\n",
      "          [0.3480, 0.3294, 0.3226, 0.0000],\n",
      "          [0.3432, 0.3313, 0.3256, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5029, 0.4971, 0.0000, 0.0000],\n",
      "          [0.3342, 0.3196, 0.3462, 0.0000],\n",
      "          [0.3323, 0.3301, 0.3376, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4997, 0.5003, 0.0000, 0.0000],\n",
      "          [0.3324, 0.3333, 0.3343, 0.0000],\n",
      "          [0.3315, 0.3371, 0.3314, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4957, 0.5043, 0.0000, 0.0000],\n",
      "          [0.3302, 0.3382, 0.3315, 0.0000],\n",
      "          [0.3378, 0.3331, 0.3291, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# 写一个 Block\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主意有 残差链接\n",
    "        # eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性）\n",
    "        # RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1\n",
    "        # RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$\n",
    "        self.layernorm_att = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.drop_att = nn.Dropout(self.dropout)\n",
    "\n",
    "        # for ffn 准备\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "        self.drop_ffn = nn.Dropout(self.dropout)\n",
    "\n",
    "    def attention_output(self, query, key, value, attention_mask=None):\n",
    "        # 计算两者相关性\n",
    "        key = key.transpose(2, 3)  # (batch, num_head, head_dim, seq)\n",
    "        att_weight = torch.matmul(query, key) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # attention mask 进行依次调整；变成 causal_attention\n",
    "        if attention_mask is not None:\n",
    "            # 变成下三角矩阵\n",
    "            attention_mask = attention_mask.tril()\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "        else:\n",
    "            # 人工构造一个下三角的 attention mask\n",
    "            attention_mask = torch.ones_like(att_weight).tril()\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        print(att_weight)\n",
    "\n",
    "        att_weight = self.drop_att(att_weight)\n",
    "\n",
    "        mid_output = torch.matmul(att_weight, value)\n",
    "        # mid_output shape is: (batch, nums_head, seq, head_dim)\n",
    "\n",
    "        mid_output = mid_output.transpose(1, 2).contiguous()\n",
    "        batch, seq, _, _ = mid_output.size()\n",
    "        mid_output = mid_output.view(batch, seq, -1)\n",
    "        output = self.o_proj(mid_output)\n",
    "        return output\n",
    "\n",
    "    def attention_block(self, X, attention_mask=None):\n",
    "        batch, seq, _ = X.size()\n",
    "        query = self.q_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "        key = self.k_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "        value = self.v_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "\n",
    "        output = self.attention_output(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return self.layernorm_att(X + output)\n",
    "\n",
    "    def ffn_block(self, X):\n",
    "        up = self.act_fn(\n",
    "            self.up_proj(X),\n",
    "        )\n",
    "        down = self.down_proj(up)\n",
    "\n",
    "        # 执行 dropout\n",
    "        down = self.drop_ffn(down)\n",
    "\n",
    "        # 进行 norm 操作\n",
    "        return self.layernorm_ffn(X + down)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X 一般假设是已经经过 embedding 的输入， (batch, seq, hidden_dim)\n",
    "        # attention_mask 一般指的是 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略\n",
    "        # shape 一般是： (batch, nums_head, seq)\n",
    "\n",
    "        att_output = self.attention_block(X, attention_mask=attention_mask)\n",
    "        ffn_output = self.ffn_block(att_output)\n",
    "        return ffn_output\n",
    "\n",
    "\n",
    "# 测试\n",
    "\n",
    "x = torch.rand(3, 4, 64)\n",
    "net = SimpleDecoder(64, 8)\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1, 8, 4, 1)\n",
    ")\n",
    "\n",
    "net(x, mask).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
